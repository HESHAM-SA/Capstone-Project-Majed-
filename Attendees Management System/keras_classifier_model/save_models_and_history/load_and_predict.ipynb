{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/10_6l7wZPW.jpg: 416x640 1 face, 80.1ms\n",
      "Speed: 7.5ms preprocess, 80.1ms inference, 1598.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/1634532.jpg: 448x640 1 face, 66.4ms\n",
      "Speed: 5.0ms preprocess, 66.4ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/2013_8_14_2_13_10_833.jpg: 384x640 3 faces, 133.3ms\n",
      "Speed: 4.3ms preprocess, 133.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/2015-09-04_20-38-28.jpg: 640x448 2 faces, 57.2ms\n",
      "Speed: 4.3ms preprocess, 57.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/2102658.jpg: 640x480 1 face, 58.5ms\n",
      "Speed: 5.9ms preprocess, 58.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/25f36deb-6cf7-475b-a8ce-0e40b63376e6_16x9_1200x676.png: 384x640 1 face, 82.6ms\n",
      "Speed: 4.1ms preprocess, 82.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/4839230_1474641026.jpg: 352x640 10 faces, 116.4ms\n",
      "Speed: 3.0ms preprocess, 116.4ms inference, 7.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/8AFD98A-D98AD981D8A7D988D8B6-D8A3D984D8AAD988D986-D8ACD988D8B2D98AD987-49b15.jpg: 416x640 1 face, 38.1ms\n",
      "Speed: 3.6ms preprocess, 38.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/8Hmeg38U_400x400.jpg: 640x640 1 face, 60.8ms\n",
      "Speed: 6.4ms preprocess, 60.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/99-140348-elton-jose-wahda-ksa_700x400.png: 384x640 1 face, 21.0ms\n",
      "Speed: 2.3ms preprocess, 21.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/B38D18B132544.jpg: 640x640 1 face, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/D8A7D984D8AAD988D986-D8AED988D8B2D98AD987-799x800.jpg: 640x640 1 face, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/D8ACD988D8B2D98AD987.jpg: 448x640 2 faces, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/e5e2d381-0115-4294-8b2f-55ba3cab612e-645x505-1.jpg: 512x640 8 faces, 62.6ms\n",
      "Speed: 2.8ms preprocess, 62.6ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/e5e2d381-0115-4294-8b2f-55ba3cab612e.jpg: 384x640 6 faces, 26.1ms\n",
      "Speed: 5.2ms preprocess, 26.1ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/hihi2-534.jpg: 640x512 1 face, 69.2ms\n",
      "Speed: 3.0ms preprocess, 69.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/HoDZ7ORqJ-qeuQjJ.jpg: 640x384 2 faces, 54.5ms\n",
      "Speed: 2.0ms preprocess, 54.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/hqdefault.jpg: 480x640 1 face, 103.2ms\n",
      "Speed: 3.7ms preprocess, 103.2ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image.jpeg: 480x640 1 face, 9.0ms\n",
      "Speed: 2.9ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image19.jpeg: 480x640 1 face, 18.2ms\n",
      "Speed: 2.6ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image20.jpeg: 480x640 1 face, 20.7ms\n",
      "Speed: 2.2ms preprocess, 20.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image21.jpeg: 640x448 2 faces, 27.2ms\n",
      "Speed: 3.2ms preprocess, 27.2ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image28.jpeg: 416x640 1 face, 19.7ms\n",
      "Speed: 3.5ms preprocess, 19.7ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image29.jpeg: 416x640 1 face, 20.5ms\n",
      "Speed: 1.9ms preprocess, 20.5ms inference, 3.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image30.jpeg: 480x640 1 face, 11.7ms\n",
      "Speed: 3.0ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image31.jpeg: 512x640 2 faces, 15.8ms\n",
      "Speed: 2.7ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image41.jpg: 448x640 5 faces, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/image8.jpeg: 512x640 1 face, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/elton/sabq2F2022-112F07ded123-5be2-49da-9515-e9b070e0713a2FIMG_1028.jpg: 640x384 2 faces, 28.2ms\n",
      "Speed: 2.5ms preprocess, 28.2ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/12043_171995.jpg: 480x640 1 face, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/1496676.jpg: 384x640 1 face, 15.6ms\n",
      "Speed: 2.4ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/199.jpg: 384x640 1 face, 14.3ms\n",
      "Speed: 2.0ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/21802_165510.jpg: 512x640 1 face, 22.7ms\n",
      "Speed: 3.1ms preprocess, 22.7ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/2296652.jpeg: 640x512 1 face, 31.5ms\n",
      "Speed: 3.0ms preprocess, 31.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/381570_100376.jpg: 640x416 1 face, 56.6ms\n",
      "Speed: 3.5ms preprocess, 56.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/381570_100377.jpg: 448x640 1 face, 26.8ms\n",
      "Speed: 3.1ms preprocess, 26.8ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/42395_120853.jpg: 448x640 1 face, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/43c41ea5-f913-4c3d-8a4f-aa05c686924b.jpeg: 384x640 1 face, 32.3ms\n",
      "Speed: 2.3ms preprocess, 32.3ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/493405.JPG.jpg: 640x480 1 face, 18.0ms\n",
      "Speed: 2.9ms preprocess, 18.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/576799_176811.jpg: 480x640 1 face, 16.7ms\n",
      "Speed: 2.6ms preprocess, 16.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/6432.jpg: 640x544 1 face, 59.2ms\n",
      "Speed: 3.0ms preprocess, 59.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/6666-6.jpg: 480x640 2 faces, 23.0ms\n",
      "Speed: 2.6ms preprocess, 23.0ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/7D984D8A7D8AAD8ADD8A7D8AF-D8A8D8AFD988D986-D985D982D8A7D8A8D984-D984D8B1D8AF.jpg: 480x640 1 face, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/9327_131269.jpg: 512x640 1 face, 18.1ms\n",
      "Speed: 2.9ms preprocess, 18.1ms inference, 3.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/950_5cdad31117.jpg: 640x640 1 face, 21.3ms\n",
      "Speed: 3.7ms preprocess, 21.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/malk/950_ba9391e706.jpg: 384x640 1 face, 21.8ms\n",
      "Speed: 2.3ms preprocess, 21.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/0-132.jpg: 384x640 1 face, 25.9ms\n",
      "Speed: 2.3ms preprocess, 25.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/124-214521-yasser-al-qahtani-saudi-hilal-history_700x400.jpg: 384x640 1 face, 23.1ms\n",
      "Speed: 2.6ms preprocess, 23.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/138-112417-alqahtani-fasting-asian-glory_700x400.jpg: 384x640 1 face, 19.2ms\n",
      "Speed: 2.4ms preprocess, 19.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/1ea60361-41d2-4791-a79e-74df9d2dd20a.jpg: 576x640 1 face, 59.2ms\n",
      "Speed: 3.4ms preprocess, 59.2ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/2-28.jpg: 640x416 5 faces, 14.2ms\n",
      "Speed: 2.5ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/21085.jpg: 640x512 1 face, 16.1ms\n",
      "Speed: 2.5ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/23316_13145.jpg: 448x640 1 face, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/2Y0wcKtU_400x400.jpg: 640x640 1 face, 37.4ms\n",
      "Speed: 4.1ms preprocess, 37.4ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/322c385c-0c5d-404e-bfdf-73e72be21cc9.jpg: 640x640 2 faces, 19.0ms\n",
      "Speed: 3.4ms preprocess, 19.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/3EF8ACE3-3F6E-4B70-9F11-91144BD93891.jpg: 448x640 1 face, 15.1ms\n",
      "Speed: 3.1ms preprocess, 15.1ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/4-27.jpg: 384x640 1 face, 37.7ms\n",
      "Speed: 3.9ms preprocess, 37.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/4021.png: 640x640 1 face, 32.7ms\n",
      "Speed: 4.9ms preprocess, 32.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/436x328_88797_257091-20121226-101124.jpg: 512x640 1 face, 32.4ms\n",
      "Speed: 3.5ms preprocess, 32.4ms inference, 4.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/45e9d1ae-e1a7-49e4-b06b-86fff2b0a668.jpeg: 384x640 1 face, 20.1ms\n",
      "Speed: 2.3ms preprocess, 20.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/4fcfbf136c2f62055f8aff5086c58093170752ae.jpg: 384x640 1 face, 19.1ms\n",
      "Speed: 2.8ms preprocess, 19.1ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/5727.jpg: 480x640 1 face, 16.9ms\n",
      "Speed: 2.8ms preprocess, 16.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/787.jpg: 640x640 1 face, 21.3ms\n",
      "Speed: 3.6ms preprocess, 21.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/800_7e8e1d0c6f.jpg: 640x448 1 face, 15.2ms\n",
      "Speed: 3.2ms preprocess, 15.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/8ACD985_D98AD8A7D8B3D8B1_D8A7D984D982D8ADD8B7D8A7D986D98A_D8B5D988D8B1D8A9_1.jpg: 448x640 (no detections), 23.0ms\n",
      "Speed: 2.8ms preprocess, 23.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/8B7D8A7D986D98A-D981D98A-D8B9D8A7D984D985-D8A7D984D8AFD8B1D8A7D985D8A7-74336.jpg: 640x640 1 face, 22.7ms\n",
      "Speed: 4.3ms preprocess, 22.7ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/8e22c36676f35d9494e8513cbf1c3c38.jpg: 544x640 (no detections), 56.7ms\n",
      "Speed: 2.7ms preprocess, 56.7ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/98-105134-yasser-qahtani-blue-flying-lover_700x400.jpg: 384x640 1 face, 21.0ms\n",
      "Speed: 2.1ms preprocess, 21.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/98AD8A7D984-D8A8D8B3D8A8D8A8-D8AAD988D8ACD98AD987D987-D8B9D8A8D8A7-1-780x470.jpg: 416x640 1 face, 18.9ms\n",
      "Speed: 3.0ms preprocess, 18.9ms inference, 4.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /mnt/c/Users/user/Desktop/therd_try/persons/yasser/_315x420_9706d9a3751568109d404e457cf3d2431549e644bce5951b5f9c7cd2c6dfaeab.jpg: 640x480 1 face, 16.0ms\n",
      "Speed: 2.7ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Face detection and cropping completed.\n"
     ]
    }
   ],
   "source": [
    "# to detect and crop faces and save it in folders \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, Colors\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_bounding_boxes(yolo_result, colors=Colors()):\n",
    "    \"\"\"Draw bounding boxes from yolo.predict() result\"\"\"\n",
    "    image = yolo_result.orig_img[..., ::-1]\n",
    "    annotate = Annotator(np.ascontiguousarray(image))\n",
    "    classes = yolo_result.boxes.cls.tolist()\n",
    "    scores = yolo_result.boxes.conf.tolist()\n",
    "    boxes = yolo_result.boxes.xyxy\n",
    "    for box, class_, score in zip(boxes, classes, scores):\n",
    "        color = np.random.randint(0, 255, 3).tolist()\n",
    "        tag = f\"{yolo_result.names[class_].title()}: {score:.0%}\"\n",
    "        annotate.box_label(box, tag, color)\n",
    "    return Image.fromarray(annotate.result())\n",
    "\n",
    "def crop_faces(yolo_result, image_path, output_path):\n",
    "    \"\"\"Crop faces from the image based on YOLO result and save them\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')  # Convert to RGB to avoid RGBA issue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image Not Found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    boxes = yolo_result.boxes.xyxy\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face = image.crop((x1, y1, x2, y2))\n",
    "        face_path = os.path.join(output_path, f\"{Path(image_path).stem}_face_{i}.jpg\")\n",
    "        face.save(face_path)\n",
    "\n",
    "# Define paths\n",
    "data_dir = '/mnt/c/Users/user/Desktop/therd_try/persons'\n",
    "output_dir = 'cropped_faces_no_labeled_players'\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO('yolov8n_face.pt')\n",
    "\n",
    "# Process each image in the dataset\n",
    "for person_folder in os.listdir(data_dir):\n",
    "    person_path = os.path.join(data_dir, person_folder)\n",
    "    if os.path.isdir(person_path):\n",
    "        person_output_path = os.path.join(output_dir, person_folder)\n",
    "        os.makedirs(person_output_path, exist_ok=True)\n",
    "\n",
    "        for image_name in os.listdir(person_path):\n",
    "            image_path = os.path.join(person_path, image_name)\n",
    "            if not os.path.isfile(image_path):\n",
    "                print(f\"Image Not Found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            result = model(image_path)[0]\n",
    "            crop_faces(result, image_path, person_output_path)\n",
    "\n",
    "print(\"Face detection and cropping completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 16:07:26.298227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 16:07:27.729856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 files belonging to 3 classes.\n",
      "Using 85 files for training.\n",
      "Using 21 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 16:07:30.359613: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:30.909343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:30.909408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:30.912479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:30.912557: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:30.912596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:31.156692: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:31.156802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:31.156842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-27 16:07:31.156947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-27 16:07:31.157130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2861 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['elton', 'malk', 'yasser']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Distorts the color distibutions of images\n",
    "class RandomColorAffine(layers.Layer):\n",
    "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "        self.brightness = brightness\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n",
    "        return config\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if training:\n",
    "            batch_size = ops.shape(images)[0]\n",
    "\n",
    "            # Same for all colors\n",
    "            brightness_scales = 1 + keras.random.uniform(\n",
    "                (batch_size, 1, 1, 1),\n",
    "                minval=-self.brightness,\n",
    "                maxval=self.brightness,\n",
    "                seed=self.seed_generator,\n",
    "            )\n",
    "            # Different for all colors\n",
    "            jitter_matrices = keras.random.uniform(\n",
    "                (batch_size, 1, 3, 3), \n",
    "                minval=-self.jitter, \n",
    "                maxval=self.jitter,\n",
    "                seed=self.seed_generator,\n",
    "            )\n",
    "\n",
    "            color_transforms = (\n",
    "                ops.tile(ops.expand_dims(ops.eye(3), axis=0), (batch_size, 1, 1, 1))\n",
    "                * brightness_scales\n",
    "                + jitter_matrices\n",
    "            )\n",
    "            images = ops.clip(ops.matmul(images, color_transforms), 0, 1)\n",
    "        return images\n",
    "\n",
    "\n",
    "labeled_train_dataset, test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    \"/mnt/c/Users/user/Desktop/therd_try/cropped_faces_no_labeled_players\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(128, 128),\n",
    "    crop_to_aspect_ratio=False,\n",
    "    pad_to_aspect_ratio=True,\n",
    "    interpolation='bilinear',\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "labeled_train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_3, built=True>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.models.load_model('./keras_model_classify.keras').backbone,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "    ]\n",
    ")\n",
    "baseline_model = keras.models.load_model('./baseline_model.keras', custom_objects={'RandomColorAffine': RandomColorAffine})\n",
    "# pretraining_model = keras.models.load_model('./pretraining_model.keras', custom_objects={'RandomColorAffine': RandomColorAffine})\n",
    "finetuning_model = keras.models.load_model('./finetuning_model.keras', custom_objects={'RandomColorAffine':RandomColorAffine})\n",
    "\n",
    "baseline_model.pop()\n",
    "finetuning_model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess(image_path):\n",
    "    img = cv2.imread(image_path)[..., ::-1]\n",
    "    img_resize = cv2.resize(img, (128, 128))\n",
    "    img = np.expand_dims(img_resize, axis=0)\n",
    "    return img\n",
    "\n",
    "def get_features(model, image):\n",
    "    if isinstance(image, str):\n",
    "        image = preprocess(image)\n",
    "    return model(image, training=True).numpy()[0]\n",
    "\n",
    "def distance(x, y):\n",
    "    # Eucleadian Distance\n",
    "    return (((x - y)**2).sum())**0.5\n",
    "\n",
    "def similarity(x, y):\n",
    "    # Cosine Similarity\n",
    "    x = x / np.linalg.norm(x)\n",
    "    y = y / np.linalg.norm(y)\n",
    "    return (x * y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity\n",
      "elton 0.087997876\n",
      "malk 0.3689572\n",
      "yasser 0.26021373\n",
      "\n",
      "elton 34.4320141530061\n",
      "malk 24.98391694584801\n",
      "yasser 29.096926014883856\n"
     ]
    }
   ],
   "source": [
    "model = finetuning_model\n",
    "\n",
    "unknown = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/elton/99-140348-elton-jose-wahda-ksa_700x400.png')\n",
    "\n",
    "elton = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/elton/2102658.jpg')\n",
    "malk = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/malk/6432.jpg')\n",
    "yasser = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/yasser/1ea60361-41d2-4791-a79e-74df9d2dd20a.jpg')\n",
    "\n",
    "print('Similarity')\n",
    "print('elton', similarity(unknown, elton))\n",
    "print('malk', similarity(unknown, malk))\n",
    "print('yasser', similarity(unknown, yasser))\n",
    "print()\n",
    "print('elton', distance(unknown, elton))\n",
    "print('malk', distance(unknown, malk))\n",
    "print('yasser', distance(unknown, yasser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity\n",
      "elton 0.5474911\n",
      "malk 0.73349345\n",
      "yasser 0.53449214\n",
      "\n",
      "elton 1.7033311911542726\n",
      "malk 1.2599505148358823\n",
      "yasser 1.952624569678853\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model\n",
    "\n",
    "unknown = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/cropped_faces_no_labeled_players/elton/8AFD98A-D98AD981D8A7D988D8B6-D8A3D984D8AAD988D986-D8ACD988D8B2D98AD987-49b15_face_0.jpg')\n",
    "\n",
    "elton = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/cropped_faces_no_labeled_players/elton/99-140348-elton-jose-wahda-ksa_700x400_face_0.jpg')\n",
    "malk = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/cropped_faces_no_labeled_players/malk/7D984D8A7D8AAD8ADD8A7D8AF-D8A8D8AFD988D986-D985D982D8A7D8A8D984-D984D8B1D8AF_face_0.jpg')\n",
    "yasser = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/cropped_faces_no_labeled_players/yasser/1ea60361-41d2-4791-a79e-74df9d2dd20a_face_0.jpg')\n",
    "\n",
    "print('Similarity')\n",
    "print('elton', similarity(unknown, elton))\n",
    "print('malk', similarity(unknown, malk))\n",
    "print('yasser', similarity(unknown, yasser))\n",
    "print()\n",
    "print('elton', distance(unknown, elton))\n",
    "print('malk', distance(unknown, malk))\n",
    "print('yasser', distance(unknown, yasser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0031505"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = finetuning_model\n",
    "\n",
    "elton1 = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/malk/43c41ea5-f913-4c3d-8a4f-aa05c686924b.jpeg')\n",
    "elton1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0031505"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elton2 = get_features(model, '/mnt/c/Users/user/Desktop/therd_try/persons/malk/43c41ea5-f913-4c3d-8a4f-aa05c686924b.jpeg')\n",
    "elton2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elton 1.0\n",
      "elton 0.0\n"
     ]
    }
   ],
   "source": [
    "print('elton', similarity(elton1, elton2))\n",
    "print('elton', distance(elton1, elton2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
